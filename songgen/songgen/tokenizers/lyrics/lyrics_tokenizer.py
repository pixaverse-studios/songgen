import os
import re
import textwrap
from functools import cached_property

import pypinyin
import torch
from hangul_romanize import Transliter
from hangul_romanize.rule import academic
from num2words import num2words
from spacy.lang.ar import Arabic
from spacy.lang.en import English
from spacy.lang.es import Spanish
from spacy.lang.ja import Japanese
from spacy.lang.zh import Chinese
from tokenizers import Tokenizer

from .zh_num2words import TextNorm as zh_num2words
from typing import Dict, List, Optional, Set, Union


#copy from https://github.com/coqui-ai/TTS/blob/dbf1a08a0d4e47fdad6172e433eeb34bc6b13b4e/TTS/tts/layers/xtts/tokenizer.py
def get_spacy_lang(lang):
    if lang == "zh":
        return Chinese()
    elif lang == "ja":
        return Japanese()
    elif lang == "ar":
        return Arabic()
    elif lang == "es":
        return Spanish()
    else:
        # For most languages, Enlish does the job
        return English()


def split_sentence(text, lang, text_split_length=250):
    """Preprocess the input text"""
    text_splits = []
    if text_split_length is not None and len(text) >= text_split_length:
        text_splits.append("")
        nlp = get_spacy_lang(lang)
        nlp.add_pipe("sentencizer")
        doc = nlp(text)
        for sentence in doc.sents:
            if len(text_splits[-1]) + len(str(sentence)) <= text_split_length:
                # if the last sentence + the current sentence is less than the text_split_length
                # then add the current sentence to the last sentence
                text_splits[-1] += " " + str(sentence)
                text_splits[-1] = text_splits[-1].lstrip()
            elif len(str(sentence)) > text_split_length:
                # if the current sentence is greater than the text_split_length
                for line in textwrap.wrap(
                    str(sentence),
                    width=text_split_length,
                    drop_whitespace=True,
                    break_on_hyphens=False,
                    tabsize=1,
                ):
                    text_splits.append(str(line))
            else:
                text_splits.append(str(sentence))

        if len(text_splits) > 1:
            if text_splits[0] == "":
                del text_splits[0]
    else:
        text_splits = [text.lstrip()]

    return text_splits


_whitespace_re = re.compile(r"\s+")

# List of (regular expression, replacement) pairs for abbreviations:
_abbreviations = {
    "en": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("mrs", "misess"),
            ("mr", "mister"),
            ("dr", "doctor"),
            ("st", "saint"),
            ("co", "company"),
            ("jr", "junior"),
            ("maj", "major"),
            ("gen", "general"),
            ("drs", "doctors"),
            ("rev", "reverend"),
            ("lt", "lieutenant"),
            ("hon", "honorable"),
            ("sgt", "sergeant"),
            ("capt", "captain"),
            ("esq", "esquire"),
            ("ltd", "limited"),
            ("col", "colonel"),
            ("ft", "fort"),
        ]
    ],
    "es": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("sra", "seÃ±ora"),
            ("sr", "seÃ±or"),
            ("dr", "doctor"),
            ("dra", "doctora"),
            ("st", "santo"),
            ("co", "compaÃ±Ã­a"),
            ("jr", "junior"),
            ("ltd", "limitada"),
        ]
    ],
    "fr": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("mme", "madame"),
            ("mr", "monsieur"),
            ("dr", "docteur"),
            ("st", "saint"),
            ("co", "compagnie"),
            ("jr", "junior"),
            ("ltd", "limitÃ©e"),
        ]
    ],
    "de": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("fr", "frau"),
            ("dr", "doktor"),
            ("st", "sankt"),
            ("co", "firma"),
            ("jr", "junior"),
        ]
    ],
    "pt": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("sra", "senhora"),
            ("sr", "senhor"),
            ("dr", "doutor"),
            ("dra", "doutora"),
            ("st", "santo"),
            ("co", "companhia"),
            ("jr", "jÃºnior"),
            ("ltd", "limitada"),
        ]
    ],
    "it": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # ("sig.ra", "signora"),
            ("sig", "signore"),
            ("dr", "dottore"),
            ("st", "santo"),
            ("co", "compagnia"),
            ("jr", "junior"),
            ("ltd", "limitata"),
        ]
    ],
    "pl": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("p", "pani"),
            ("m", "pan"),
            ("dr", "doktor"),
            ("sw", "Å›wiÄ™ty"),
            ("jr", "junior"),
        ]
    ],
    "ar": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # There are not many common abbreviations in Arabic as in English.
        ]
    ],
    "zh": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # Chinese doesn't typically use abbreviations in the same way as Latin-based scripts.
        ]
    ],
    "cs": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dr", "doktor"),  # doctor
            ("ing", "inÅ¾enÃ½r"),  # engineer
            ("p", "pan"),  # Could also map to pani for woman but no easy way to do it
            # Other abbreviations would be specialized and not as common.
        ]
    ],
    "ru": [
        (re.compile("\\b%s\\b" % x[0], re.IGNORECASE), x[1])
        for x in [
            ("Ğ³-Ğ¶Ğ°", "Ğ³Ğ¾ÑĞ¿Ğ¾Ğ¶Ğ°"),  # Mrs.
            ("Ğ³-Ğ½", "Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ¸Ğ½"),  # Mr.
            ("Ğ´-Ñ€", "Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€"),  # doctor
            # Other abbreviations are less common or specialized.
        ]
    ],
    "nl": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dhr", "de heer"),  # Mr.
            ("mevr", "mevrouw"),  # Mrs.
            ("dr", "dokter"),  # doctor
            ("jhr", "jonkheer"),  # young lord or nobleman
            # Dutch uses more abbreviations, but these are the most common ones.
        ]
    ],
    "tr": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("b", "bay"),  # Mr.
            ("byk", "bÃ¼yÃ¼k"),  # bÃ¼yÃ¼k
            ("dr", "doktor"),  # doctor
            # Add other Turkish abbreviations here if needed.
        ]
    ],
    "hu": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dr", "doktor"),  # doctor
            ("b", "bÃ¡csi"),  # Mr.
            ("nÅ‘v", "nÅ‘vÃ©r"),  # nurse
            # Add other Hungarian abbreviations here if needed.
        ]
    ],
    "ko": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # Korean doesn't typically use abbreviations in the same way as Latin-based scripts.
        ]
    ],
}


def expand_abbreviations_multilingual(text, lang="en"):
    for regex, replacement in _abbreviations[lang]:
        text = re.sub(regex, replacement, text)
    return text


_symbols_multilingual = {
    "en": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " and "),
            ("@", " at "),
            ("%", " percent "),
            ("#", " hash "),
            ("$", " dollar "),
            ("Â£", " pound "),
            ("Â°", " degree "),
        ]
    ],
    "es": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " y "),
            ("@", " arroba "),
            ("%", " por ciento "),
            ("#", " numeral "),
            ("$", " dolar "),
            ("Â£", " libra "),
            ("Â°", " grados "),
        ]
    ],
    "fr": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " et "),
            ("@", " arobase "),
            ("%", " pour cent "),
            ("#", " diÃ¨se "),
            ("$", " dollar "),
            ("Â£", " livre "),
            ("Â°", " degrÃ©s "),
        ]
    ],
    "de": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " und "),
            ("@", " at "),
            ("%", " prozent "),
            ("#", " raute "),
            ("$", " dollar "),
            ("Â£", " pfund "),
            ("Â°", " grad "),
        ]
    ],
    "pt": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " e "),
            ("@", " arroba "),
            ("%", " por cento "),
            ("#", " cardinal "),
            ("$", " dÃ³lar "),
            ("Â£", " libra "),
            ("Â°", " graus "),
        ]
    ],
    "it": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " e "),
            ("@", " chiocciola "),
            ("%", " per cento "),
            ("#", " cancelletto "),
            ("$", " dollaro "),
            ("Â£", " sterlina "),
            ("Â°", " gradi "),
        ]
    ],
    "pl": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " i "),
            ("@", " maÅ‚pa "),
            ("%", " procent "),
            ("#", " krzyÅ¼yk "),
            ("$", " dolar "),
            ("Â£", " funt "),
            ("Â°", " stopnie "),
        ]
    ],
    "ar": [
        # Arabic
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " Ùˆ "),
            ("@", " Ø¹Ù„Ù‰ "),
            ("%", " ÙÙŠ Ø§Ù„Ù…Ø¦Ø© "),
            ("#", " Ø±Ù‚Ù… "),
            ("$", " Ø¯ÙˆÙ„Ø§Ø± "),
            ("Â£", " Ø¬Ù†ÙŠÙ‡ "),
            ("Â°", " Ø¯Ø±Ø¬Ø© "),
        ]
    ],
    "zh": [
        # Chinese
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " å’Œ "),
            ("@", " åœ¨ "),
            ("%", " ç™¾åˆ†ä¹‹ "),
            ("#", " å· "),
            ("$", " ç¾å…ƒ "),
            ("Â£", " è‹±é•‘ "),
            ("Â°", " åº¦ "),
        ]
    ],
    "cs": [
        # Czech
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " a "),
            ("@", " na "),
            ("%", " procento "),
            ("#", " kÅ™Ã­Å¾ek "),
            ("$", " dolar "),
            ("Â£", " libra "),
            ("Â°", " stupnÄ› "),
        ]
    ],
    "ru": [
        # Russian
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " Ğ¸ "),
            ("@", " ÑĞ¾Ğ±Ğ°ĞºĞ° "),
            ("%", " Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ¾Ğ² "),
            ("#", " Ğ½Ğ¾Ğ¼ĞµÑ€ "),
            ("$", " Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€ "),
            ("Â£", " Ñ„ÑƒĞ½Ñ‚ "),
            ("Â°", " Ğ³Ñ€Ğ°Ğ´ÑƒÑ "),
        ]
    ],
    "nl": [
        # Dutch
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " en "),
            ("@", " bij "),
            ("%", " procent "),
            ("#", " hekje "),
            ("$", " dollar "),
            ("Â£", " pond "),
            ("Â°", " graden "),
        ]
    ],
    "tr": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " ve "),
            ("@", " at "),
            ("%", " yÃ¼zde "),
            ("#", " diyez "),
            ("$", " dolar "),
            ("Â£", " sterlin "),
            ("Â°", " derece "),
        ]
    ],
    "hu": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " Ã©s "),
            ("@", " kukac "),
            ("%", " szÃ¡zalÃ©k "),
            ("#", " kettÅ‘skereszt "),
            ("$", " dollÃ¡r "),
            ("Â£", " font "),
            ("Â°", " fok "),
        ]
    ],
    "ko": [
        # Korean
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " ê·¸ë¦¬ê³  "),
            ("@", " ì— "),
            ("%", " í¼ì„¼íŠ¸ "),
            ("#", " ë²ˆí˜¸ "),
            ("$", " ë‹¬ëŸ¬ "),
            ("Â£", " íŒŒìš´ë“œ "),
            ("Â°", " ë„ "),
        ]
    ],
}


def expand_symbols_multilingual(text, lang="en"):
    for regex, replacement in _symbols_multilingual[lang]:
        text = re.sub(regex, replacement, text)
        text = text.replace("  ", " ")  # Ensure there are no double spaces
    return text.strip()


_ordinal_re = {
    "en": re.compile(r"([0-9]+)(st|nd|rd|th)"),
    "es": re.compile(r"([0-9]+)(Âº|Âª|er|o|a|os|as)"),
    "fr": re.compile(r"([0-9]+)(Âº|Âª|er|re|e|Ã¨me)"),
    "de": re.compile(r"([0-9]+)(st|nd|rd|th|Âº|Âª|\.(?=\s|$))"),
    "pt": re.compile(r"([0-9]+)(Âº|Âª|o|a|os|as)"),
    "it": re.compile(r"([0-9]+)(Âº|Â°|Âª|o|a|i|e)"),
    "pl": re.compile(r"([0-9]+)(Âº|Âª|st|nd|rd|th)"),
    "ar": re.compile(r"([0-9]+)(ÙˆÙ†|ÙŠÙ†|Ø«|Ø±|Ù‰)"),
    "cs": re.compile(r"([0-9]+)\.(?=\s|$)"),  # In Czech, a dot is often used after the number to indicate ordinals.
    "ru": re.compile(r"([0-9]+)(-Ğ¹|-Ñ|-Ğµ|-Ğ¾Ğµ|-ÑŒĞµ|-Ğ³Ğ¾)"),
    "nl": re.compile(r"([0-9]+)(de|ste|e)"),
    "tr": re.compile(r"([0-9]+)(\.|inci|nci|uncu|Ã¼ncÃ¼|\.)"),
    "hu": re.compile(r"([0-9]+)(\.|adik|edik|odik|edik|Ã¶dik|Ã¶dike|ik)"),
    "ko": re.compile(r"([0-9]+)(ë²ˆì§¸|ë²ˆ|ì°¨|ì§¸)"),
}
_number_re = re.compile(r"[0-9]+")
_currency_re = {
    "USD": re.compile(r"((\$[0-9\.\,]*[0-9]+)|([0-9\.\,]*[0-9]+\$))"),
    "GBP": re.compile(r"((Â£[0-9\.\,]*[0-9]+)|([0-9\.\,]*[0-9]+Â£))"),
    "EUR": re.compile(r"(([0-9\.\,]*[0-9]+â‚¬)|((â‚¬[0-9\.\,]*[0-9]+)))"),
}

_comma_number_re = re.compile(r"\b\d{1,3}(,\d{3})*(\.\d+)?\b")
_dot_number_re = re.compile(r"\b\d{1,3}(.\d{3})*(\,\d+)?\b")
_decimal_number_re = re.compile(r"([0-9]+[.,][0-9]+)")


def _remove_commas(m):
    text = m.group(0)
    if "," in text:
        text = text.replace(",", "")
    return text


def _remove_dots(m):
    text = m.group(0)
    if "." in text:
        text = text.replace(".", "")
    return text


def _expand_decimal_point(m, lang="en"):
    amount = m.group(1).replace(",", ".")
    return num2words(float(amount), lang=lang if lang != "cs" else "cz")


def _expand_currency(m, lang="en", currency="USD"):
    amount = float((re.sub(r"[^\d.]", "", m.group(0).replace(",", "."))))
    full_amount = num2words(amount, to="currency", currency=currency, lang=lang if lang != "cs" else "cz")

    and_equivalents = {
        "en": ", ",
        "es": " con ",
        "fr": " et ",
        "de": " und ",
        "pt": " e ",
        "it": " e ",
        "pl": ", ",
        "cs": ", ",
        "ru": ", ",
        "nl": ", ",
        "ar": ", ",
        "tr": ", ",
        "hu": ", ",
        "ko": ", ",
    }

    if amount.is_integer():
        last_and = full_amount.rfind(and_equivalents[lang])
        if last_and != -1:
            full_amount = full_amount[:last_and]

    return full_amount


def _expand_ordinal(m, lang="en"):
    return num2words(int(m.group(1)), ordinal=True, lang=lang if lang != "cs" else "cz")


def _expand_number(m, lang="en"):
    return num2words(int(m.group(0)), lang=lang if lang != "cs" else "cz")


def expand_numbers_multilingual(text, lang="en"):
    if lang == "zh":
        text = zh_num2words()(text)
    else:
        if lang in ["en", "ru"]:
            text = re.sub(_comma_number_re, _remove_commas, text)
        else:
            text = re.sub(_dot_number_re, _remove_dots, text)
        try:
            text = re.sub(_currency_re["GBP"], lambda m: _expand_currency(m, lang, "GBP"), text)
            text = re.sub(_currency_re["USD"], lambda m: _expand_currency(m, lang, "USD"), text)
            text = re.sub(_currency_re["EUR"], lambda m: _expand_currency(m, lang, "EUR"), text)
        except:
            pass
        if lang != "tr":
            text = re.sub(_decimal_number_re, lambda m: _expand_decimal_point(m, lang), text)
        text = re.sub(_ordinal_re[lang], lambda m: _expand_ordinal(m, lang), text)
        text = re.sub(_number_re, lambda m: _expand_number(m, lang), text)
    return text


def lowercase(text):
    return text.lower()


def collapse_whitespace(text):
    return re.sub(_whitespace_re, " ", text)


def multilingual_cleaners(text, lang):
    text = text.replace('"', "")
    if lang == "tr":
        text = text.replace("Ä°", "i")
        text = text.replace("Ã–", "Ã¶")
        text = text.replace("Ãœ", "Ã¼")
    text = lowercase(text)
    text = expand_numbers_multilingual(text, lang)
    text = expand_abbreviations_multilingual(text, lang)
    text = expand_symbols_multilingual(text, lang=lang)
    text = collapse_whitespace(text)
    return text


def basic_cleaners(text):
    """Basic pipeline that lowercases and collapses whitespace without transliteration."""
    text = lowercase(text)
    text = collapse_whitespace(text)
    return text


def chinese_transliterate(text):
    return "".join(
        [p[0] for p in pypinyin.pinyin(text, style=pypinyin.Style.TONE3, heteronym=False, neutral_tone_with_five=True)]
    )


def japanese_cleaners(text, katsu):
    text = katsu.romaji(text)
    text = lowercase(text)
    return text


def korean_transliterate(text):
    r = Transliter(academic)
    return r.translit(text)


DEFAULT_VOCAB_FILE = os.path.join(os.path.dirname(os.path.realpath(__file__)), "vocab.json")


class VoiceBpeTokenizer:
    def __init__(self, vocab_file=DEFAULT_VOCAB_FILE):
        self.tokenizer = None
        if vocab_file is not None:
            self.tokenizer = Tokenizer.from_file(vocab_file)
        self.char_limits = {
            "en": 10000,
            "de": 253,
            "fr": 273,
            "es": 239,
            "it": 213,
            "pt": 203,
            "pl": 224,
            "zh": 82,
            "ar": 166,
            "cs": 186,
            "ru": 182,
            "nl": 251,
            "tr": 226,
            "ja": 71,
            "hu": 224,
            "ko": 95,
        }

    @cached_property
    def katsu(self):
        import cutlet

        return cutlet.Cutlet()

    def check_input_length(self, txt, lang):
        lang = lang.split("-")[0]  # remove the region
        limit = self.char_limits.get(lang, 250)
        if len(txt) > limit:
            print(
                f"[!] Warning: The text length exceeds the character limit of {limit} for language '{lang}', this might cause truncated audio."
            )

    def clean_lyrics(self, text: str) -> str:
        """Clean lyrics by removing timestamps, music notations, and other special characters.
        
        Args:
            text (str): Input lyrics text
            
        Returns:
            str: Cleaned lyrics text
        """
        # Remove timestamps [MM:SS]
        text = re.sub(r'\[\d+:\d+\]', '', text)
        
        # Remove music notations and emojis
        text = re.sub(r'[â™ªâ™«â™¬ğŸµğŸ¶]', '', text)
        text = re.sub(r'\[Music\]|\[Applause\]|\[Background Music\]', '', text)
        
        # Remove parenthetical descriptions like (Chorus) or (Verse 1)
        text = re.sub(r'\([^)]*\)', '', text)
        
        # Remove square bracket descriptions [Chorus] or [Verse 1]
        text = re.sub(r'\[[^\]]*\]', '', text)
        
        # Collapse multiple newlines into single newline
        text = re.sub(r'\n+', '\n', text)
        
        # Collapse multiple spaces into single space
        text = re.sub(r'\s+', ' ', text)
        
        # Strip leading/trailing whitespace
        text = text.strip()
        
        return text

    def preprocess_text(self, txt, lang):
        if lang in {"ar", "cs", "de", "en", "es", "fr", "hu", "it", "nl", "pl", "pt", "ru", "tr", "zh", "ko"}:
            # Clean lyrics before other processing
            txt = self.clean_lyrics(txt)
            txt = multilingual_cleaners(txt, lang)
            if lang == "zh":
                txt = chinese_transliterate(txt)
            if lang == "ko":
                txt = korean_transliterate(txt)
        elif lang == "ja":
            txt = self.clean_lyrics(txt)
            txt = japanese_cleaners(txt, self.katsu)
        elif lang == "hi":
            # @manmay will implement this
            txt = self.clean_lyrics(txt)
            txt = basic_cleaners(txt)
        else:
            raise NotImplementedError(f"Language '{lang}' is not supported.")
        return txt

    def encode(self, txt, lang):
        lang = lang.split("-")[0]  # remove the region
        self.check_input_length(txt, lang)
        txt = self.preprocess_text(txt, lang)
        lang = "zh-cn" if lang == "zh" else lang
        txt = f"[{lang}]{txt}"
        txt = txt.replace(" ", "[SPACE]")
        return self.tokenizer.encode(txt).ids

    def decode(self, seq, skip_special_tokens=False):
        if isinstance(seq, torch.Tensor):
            seq = seq.cpu().numpy()
        txt = self.tokenizer.decode(seq, skip_special_tokens=False).replace(" ", "")
        txt = txt.replace("[SPACE]", " ")
        txt = txt.replace("[STOP]", "")
        # txt = txt.replace("[UNK]", "")
        return txt
    

    #copy from https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L3936
    def batch_decode(
        self,
        sequences: Union[List[int], List[List[int]], "np.ndarray", "torch.Tensor", "tf.Tensor"],
        skip_special_tokens: bool = False,
    ) -> List[str]:
        """
        Convert a list of lists of token ids into a list of strings by calling decode.

        Args:
            sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):
                List of tokenized input ids. Can be obtained using the `__call__` method.
            skip_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not to remove special tokens in the decoding.
            kwargs (additional keyword arguments, *optional*):
                Will be passed to the underlying model specific decode method.

        Returns:
            `List[str]`: The list of decoded sentences.
        """
        return [
            self.decode(seq)
            for seq in sequences
        ]
    
    #https://github.com/coqui-ai/TTS/blob/dev/TTS/tts/layers/xtts/trainer/dataset.py#L202
    # def pad(self): 

    def __len__(self):
        return self.tokenizer.get_vocab_size()

    def get_number_tokens(self):
        return max(self.tokenizer.get_vocab().values()) + 1


def test_expand_numbers_multilingual():
    test_cases = [
        # English
        ("In 12.5 seconds.", "In twelve point five seconds.", "en"),
        ("There were 50 soldiers.", "There were fifty soldiers.", "en"),
        ("This is a 1st test", "This is a first test", "en"),
        ("That will be $20 sir.", "That will be twenty dollars sir.", "en"),
        ("That will be 20â‚¬ sir.", "That will be twenty euro sir.", "en"),
        ("That will be 20.15â‚¬ sir.", "That will be twenty euro, fifteen cents sir.", "en"),
        ("That's 100,000.5.", "That's one hundred thousand point five.", "en"),
        # French
        ("En 12,5 secondes.", "En douze virgule cinq secondes.", "fr"),
        ("Il y avait 50 soldats.", "Il y avait cinquante soldats.", "fr"),
        ("Ceci est un 1er test", "Ceci est un premier test", "fr"),
        ("Cela vous fera $20 monsieur.", "Cela vous fera vingt dollars monsieur.", "fr"),
        ("Cela vous fera 20â‚¬ monsieur.", "Cela vous fera vingt euros monsieur.", "fr"),
        ("Cela vous fera 20,15â‚¬ monsieur.", "Cela vous fera vingt euros et quinze centimes monsieur.", "fr"),
        ("Ce sera 100.000,5.", "Ce sera cent mille virgule cinq.", "fr"),
        # German
        ("In 12,5 Sekunden.", "In zwÃ¶lf Komma fÃ¼nf Sekunden.", "de"),
        ("Es gab 50 Soldaten.", "Es gab fÃ¼nfzig Soldaten.", "de"),
        ("Dies ist ein 1. Test", "Dies ist ein erste Test", "de"),  # Issue with gender
        ("Das macht $20 Herr.", "Das macht zwanzig Dollar Herr.", "de"),
        ("Das macht 20â‚¬ Herr.", "Das macht zwanzig Euro Herr.", "de"),
        ("Das macht 20,15â‚¬ Herr.", "Das macht zwanzig Euro und fÃ¼nfzehn Cent Herr.", "de"),
        # Spanish
        ("En 12,5 segundos.", "En doce punto cinco segundos.", "es"),
        ("HabÃ­a 50 soldados.", "HabÃ­a cincuenta soldados.", "es"),
        ("Este es un 1er test", "Este es un primero test", "es"),
        ("Eso le costarÃ¡ $20 seÃ±or.", "Eso le costarÃ¡ veinte dÃ³lares seÃ±or.", "es"),
        ("Eso le costarÃ¡ 20â‚¬ seÃ±or.", "Eso le costarÃ¡ veinte euros seÃ±or.", "es"),
        ("Eso le costarÃ¡ 20,15â‚¬ seÃ±or.", "Eso le costarÃ¡ veinte euros con quince cÃ©ntimos seÃ±or.", "es"),
        # Italian
        ("In 12,5 secondi.", "In dodici virgola cinque secondi.", "it"),
        ("C'erano 50 soldati.", "C'erano cinquanta soldati.", "it"),
        ("Questo Ã¨ un 1Â° test", "Questo Ã¨ un primo test", "it"),
        ("Ti costerÃ  $20 signore.", "Ti costerÃ  venti dollari signore.", "it"),
        ("Ti costerÃ  20â‚¬ signore.", "Ti costerÃ  venti euro signore.", "it"),
        ("Ti costerÃ  20,15â‚¬ signore.", "Ti costerÃ  venti euro e quindici centesimi signore.", "it"),
        # Portuguese
        ("Em 12,5 segundos.", "Em doze vÃ­rgula cinco segundos.", "pt"),
        ("Havia 50 soldados.", "Havia cinquenta soldados.", "pt"),
        ("Este Ã© um 1Âº teste", "Este Ã© um primeiro teste", "pt"),
        ("Isso custarÃ¡ $20 senhor.", "Isso custarÃ¡ vinte dÃ³lares senhor.", "pt"),
        ("Isso custarÃ¡ 20â‚¬ senhor.", "Isso custarÃ¡ vinte euros senhor.", "pt"),
        (
            "Isso custarÃ¡ 20,15â‚¬ senhor.",
            "Isso custarÃ¡ vinte euros e quinze cÃªntimos senhor.",
            "pt",
        ),  # "cÃªntimos" should be "centavos" num2words issue
        # Polish
        ("W 12,5 sekundy.", "W dwanaÅ›cie przecinek piÄ™Ä‡ sekundy.", "pl"),
        ("ByÅ‚o 50 Å¼oÅ‚nierzy.", "ByÅ‚o piÄ™Ä‡dziesiÄ…t Å¼oÅ‚nierzy.", "pl"),
        ("To bÄ™dzie kosztowaÄ‡ 20â‚¬ panie.", "To bÄ™dzie kosztowaÄ‡ dwadzieÅ›cia euro panie.", "pl"),
        ("To bÄ™dzie kosztowaÄ‡ 20,15â‚¬ panie.", "To bÄ™dzie kosztowaÄ‡ dwadzieÅ›cia euro, piÄ™tnaÅ›cie centÃ³w panie.", "pl"),
        # Arabic
        ("ÙÙŠ Ø§Ù„Ù€ 12,5 Ø«Ø§Ù†ÙŠØ©.", "ÙÙŠ Ø§Ù„Ù€ Ø§Ø«Ù†Ø§ Ø¹Ø´Ø±  , Ø®Ù…Ø³ÙˆÙ† Ø«Ø§Ù†ÙŠØ©.", "ar"),
        ("ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ 50 Ø¬Ù†Ø¯ÙŠÙ‹Ø§.", "ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø®Ù…Ø³ÙˆÙ† Ø¬Ù†Ø¯ÙŠÙ‹Ø§.", "ar"),
        # ("Ø³ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© $20 ÙŠØ§ Ø³ÙŠØ¯.", 'Ø³ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¹Ø´Ø±ÙˆÙ† Ø¯ÙˆÙ„Ø§Ø± ÙŠØ§ Ø³ÙŠØ¯.', 'ar'), # $ and â‚¬ are mising from num2words
        # ("Ø³ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© 20â‚¬ ÙŠØ§ Ø³ÙŠØ¯.", 'Ø³ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¹Ø´Ø±ÙˆÙ† ÙŠÙˆØ±Ùˆ ÙŠØ§ Ø³ÙŠØ¯.', 'ar'),
        # Czech
        ("Za 12,5 vteÅ™iny.", "Za dvanÃ¡ct celÃ¡ pÄ›t vteÅ™iny.", "cs"),
        ("Bylo tam 50 vojÃ¡kÅ¯.", "Bylo tam padesÃ¡t vojÃ¡kÅ¯.", "cs"),
        ("To bude stÃ¡t 20â‚¬ pane.", "To bude stÃ¡t dvacet euro pane.", "cs"),
        ("To bude 20.15â‚¬ pane.", "To bude dvacet euro, patnÃ¡ct centÅ¯ pane.", "cs"),
        # Russian
        ("Ğ§ĞµÑ€ĞµĞ· 12.5 ÑĞµĞºÑƒĞ½Ğ´Ñ‹.", "Ğ§ĞµÑ€ĞµĞ· Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿ÑÑ‚Ğ°Ñ Ğ¿ÑÑ‚ÑŒ ÑĞµĞºÑƒĞ½Ğ´Ñ‹.", "ru"),
        ("Ğ¢Ğ°Ğ¼ Ğ±Ñ‹Ğ»Ğ¾ 50 ÑĞ¾Ğ»Ğ´Ğ°Ñ‚.", "Ğ¢Ğ°Ğ¼ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿ÑÑ‚ÑŒĞ´ĞµÑÑÑ‚ ÑĞ¾Ğ»Ğ´Ğ°Ñ‚.", "ru"),
        ("Ğ­Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ 20.15â‚¬ ÑÑÑ€.", "Ğ­Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ²Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ ĞµĞ²Ñ€Ğ¾, Ğ¿ÑÑ‚Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ Ñ†ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑÑ€.", "ru"),
        ("Ğ­Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ ÑÑ‚Ğ¾Ğ¸Ñ‚ÑŒ 20â‚¬ Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ¸Ğ½.", "Ğ­Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ ÑÑ‚Ğ¾Ğ¸Ñ‚ÑŒ Ğ´Ğ²Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ ĞµĞ²Ñ€Ğ¾ Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ¸Ğ½.", "ru"),
        # Dutch
        ("In 12,5 seconden.", "In twaalf komma vijf seconden.", "nl"),
        ("Er waren 50 soldaten.", "Er waren vijftig soldaten.", "nl"),
        ("Dat wordt dan $20 meneer.", "Dat wordt dan twintig dollar meneer.", "nl"),
        ("Dat wordt dan 20â‚¬ meneer.", "Dat wordt dan twintig euro meneer.", "nl"),
        # Chinese (Simplified)
        ("åœ¨12.5ç§’å†…", "åœ¨åäºŒç‚¹äº”ç§’å†…", "zh"),
        ("æœ‰50åå£«å…µ", "æœ‰äº”ååå£«å…µ", "zh"),
        # ("é‚£å°†æ˜¯$20å…ˆç”Ÿ", 'é‚£å°†æ˜¯äºŒåç¾å…ƒå…ˆç”Ÿ', 'zh'), currency doesn't work
        # ("é‚£å°†æ˜¯20â‚¬å…ˆç”Ÿ", 'é‚£å°†æ˜¯äºŒåæ¬§å…ƒå…ˆç”Ÿ', 'zh'),
        # Turkish
        # ("12,5 saniye iÃ§inde.", 'On iki virgÃ¼l beÅŸ saniye iÃ§inde.', 'tr'), # decimal doesn't work for TR
        ("50 asker vardÄ±.", "elli asker vardÄ±.", "tr"),
        ("Bu 1. test", "Bu birinci test", "tr"),
        # ("Bu 100.000,5.", 'Bu yÃ¼z bin virgÃ¼l beÅŸ.', 'tr'),
        # Hungarian
        ("12,5 mÃ¡sodperc alatt.", "tizenkettÅ‘ egÃ©sz Ã¶t tized mÃ¡sodperc alatt.", "hu"),
        ("50 katona volt.", "Ã¶tven katona volt.", "hu"),
        ("Ez az 1. teszt", "Ez az elsÅ‘ teszt", "hu"),
        # Korean
        ("12.5 ì´ˆ ì•ˆì—.", "ì‹­ì´ ì  ë‹¤ì„¯ ì´ˆ ì•ˆì—.", "ko"),
        ("50 ëª…ì˜ ë³‘ì‚¬ê°€ ìˆì—ˆë‹¤.", "ì˜¤ì‹­ ëª…ì˜ ë³‘ì‚¬ê°€ ìˆì—ˆë‹¤.", "ko"),
        ("ì´ê²ƒì€ 1 ë²ˆì§¸ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤", "ì´ê²ƒì€ ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤", "ko"),
    ]
    for a, b, lang in test_cases:
        out = expand_numbers_multilingual(a, lang=lang)
        assert out == b, f"'{out}' vs '{b}'"


def test_abbreviations_multilingual():
    test_cases = [
        # English
        ("Hello Mr. Smith.", "Hello mister Smith.", "en"),
        ("Dr. Jones is here.", "doctor Jones is here.", "en"),
        # Spanish
        ("Hola Sr. Garcia.", "Hola seÃ±or Garcia.", "es"),
        ("La Dra. Martinez es muy buena.", "La doctora Martinez es muy buena.", "es"),
        # French
        ("Bonjour Mr. Dupond.", "Bonjour monsieur Dupond.", "fr"),
        ("Mme. Moreau est absente aujourd'hui.", "madame Moreau est absente aujourd'hui.", "fr"),
        # German
        ("Frau Dr. MÃ¼ller ist sehr klug.", "Frau doktor MÃ¼ller ist sehr klug.", "de"),
        # Portuguese
        ("OlÃ¡ Sr. Silva.", "OlÃ¡ senhor Silva.", "pt"),
        ("Dra. Costa, vocÃª estÃ¡ disponÃ­vel?", "doutora Costa, vocÃª estÃ¡ disponÃ­vel?", "pt"),
        # Italian
        ("Buongiorno, Sig. Rossi.", "Buongiorno, signore Rossi.", "it"),
        # ("Sig.ra Bianchi, posso aiutarti?", 'signora Bianchi, posso aiutarti?', 'it'), # Issue with matching that pattern
        # Polish
        ("DzieÅ„ dobry, P. Kowalski.", "DzieÅ„ dobry, pani Kowalski.", "pl"),
        ("M. Nowak, czy mogÄ™ zadaÄ‡ pytanie?", "pan Nowak, czy mogÄ™ zadaÄ‡ pytanie?", "pl"),
        # Czech
        ("P. NovÃ¡k", "pan NovÃ¡k", "cs"),
        ("Dr. VojtÄ›ch", "doktor VojtÄ›ch", "cs"),
        # Dutch
        ("Dhr. Jansen", "de heer Jansen", "nl"),
        ("Mevr. de Vries", "mevrouw de Vries", "nl"),
        # Russian
        ("Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ Ğ“-Ğ½ Ğ˜Ğ²Ğ°Ğ½Ğ¾Ğ².", "Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ¸Ğ½ Ğ˜Ğ²Ğ°Ğ½Ğ¾Ğ².", "ru"),
        ("Ğ”-Ñ€ Ğ¡Ğ¼Ğ¸Ñ€Ğ½Ğ¾Ğ² Ğ·Ğ´ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ²Ğ°Ñ.", "Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€ Ğ¡Ğ¼Ğ¸Ñ€Ğ½Ğ¾Ğ² Ğ·Ğ´ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ²Ğ°Ñ.", "ru"),
        # Turkish
        ("Merhaba B. YÄ±lmaz.", "Merhaba bay YÄ±lmaz.", "tr"),
        ("Dr. AyÅŸe burada.", "doktor AyÅŸe burada.", "tr"),
        # Hungarian
        ("Dr. SzabÃ³ itt van.", "doktor SzabÃ³ itt van.", "hu"),
    ]

    for a, b, lang in test_cases:
        out = expand_abbreviations_multilingual(a, lang=lang)
        assert out == b, f"'{out}' vs '{b}'"


def test_symbols_multilingual():
    test_cases = [
        ("I have 14% battery", "I have 14 percent battery", "en"),
        ("Te veo @ la fiesta", "Te veo arroba la fiesta", "es"),
        ("J'ai 14Â° de fiÃ¨vre", "J'ai 14 degrÃ©s de fiÃ¨vre", "fr"),
        ("Die Rechnung betrÃ¤gt Â£ 20", "Die Rechnung betrÃ¤gt pfund 20", "de"),
        ("O meu email Ã© ana&joao@gmail.com", "O meu email Ã© ana e joao arroba gmail.com", "pt"),
        ("linguaggio di programmazione C#", "linguaggio di programmazione C cancelletto", "it"),
        ("Moja temperatura to 36.6Â°", "Moja temperatura to 36.6 stopnie", "pl"),
        ("MÃ¡m 14% baterie", "MÃ¡m 14 procento baterie", "cs"),
        ("TÄ›Å¡Ã­m se na tebe @ party", "TÄ›Å¡Ã­m se na tebe na party", "cs"),
        ("Ğ£ Ğ¼ĞµĞ½Ñ 14% Ğ·Ğ°Ñ€ÑĞ´Ğ°", "Ğ£ Ğ¼ĞµĞ½Ñ 14 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ°Ñ€ÑĞ´Ğ°", "ru"),
        ("Ğ¯ Ğ±ÑƒĞ´Ñƒ @ Ğ´Ğ¾Ğ¼Ğ°", "Ğ¯ Ğ±ÑƒĞ´Ñƒ ÑĞ¾Ğ±Ğ°ĞºĞ° Ğ´Ğ¾Ğ¼Ğ°", "ru"),
        ("Ik heb 14% batterij", "Ik heb 14 procent batterij", "nl"),
        ("Ik zie je @ het feest", "Ik zie je bij het feest", "nl"),
        ("Ù„Ø¯ÙŠ 14% ÙÙŠ Ø§Ù„Ø¨Ø·Ø§Ø±ÙŠØ©", "Ù„Ø¯ÙŠ 14 ÙÙŠ Ø§Ù„Ù…Ø¦Ø© ÙÙŠ Ø§Ù„Ø¨Ø·Ø§Ø±ÙŠØ©", "ar"),
        ("æˆ‘çš„ç”µé‡ä¸º 14%", "æˆ‘çš„ç”µé‡ä¸º 14 ç™¾åˆ†ä¹‹", "zh"),
        ("Pilim %14 dolu.", "Pilim yÃ¼zde 14 dolu.", "tr"),
        ("Az akkumulÃ¡torom tÃ¶ltÃ¶ttsÃ©ge 14%", "Az akkumulÃ¡torom tÃ¶ltÃ¶ttsÃ©ge 14 szÃ¡zalÃ©k", "hu"),
        ("ë°°í„°ë¦¬ ì”ëŸ‰ì´ 14%ì…ë‹ˆë‹¤.", "ë°°í„°ë¦¬ ì”ëŸ‰ì´ 14 í¼ì„¼íŠ¸ì…ë‹ˆë‹¤.", "ko"),
    ]

    for a, b, lang in test_cases:
        out = expand_symbols_multilingual(a, lang=lang)
        assert out == b, f"'{out}' vs '{b}'"


if __name__ == "__main__":
    test_expand_numbers_multilingual()
    test_abbreviations_multilingual()
    test_symbols_multilingual()